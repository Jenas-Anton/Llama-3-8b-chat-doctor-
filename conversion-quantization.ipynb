{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":200209297,"sourceType":"kernelVersion"},{"sourceId":200266010,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/working\n!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n%cd /kaggle/working/llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 conda run -n base make -j > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-10T05:47:29.401192Z","iopub.execute_input":"2024-10-10T05:47:29.402077Z","iopub.status.idle":"2024-10-10T05:57:48.326673Z","shell.execute_reply.started":"2024-10-10T05:47:29.402030Z","shell.execute_reply":"2024-10-10T05:57:48.325452Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'llama.cpp'...\nremote: Enumerating objects: 1115, done.\u001b[K\nremote: Counting objects: 100% (1115/1115), done.\u001b[K\nremote: Compressing objects: 100% (854/854), done.\u001b[K\nremote: Total 1115 (delta 259), reused 663 (delta 213), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (1115/1115), 17.83 MiB | 10.18 MiB/s, done.\nResolving deltas: 100% (259/259), done.\n/kaggle/working/llama.cpp\nIn file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n                 from /usr/local/cuda/include/cub/config.cuh:35,\n                 from /usr/local/cuda/include/cub/cub.cuh:37,\n                 from ggml/src/ggml-cuda/sum.cu:8:\n/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \nIn file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n                 from /usr/local/cuda/include/thrust/type_traits/integer_sequence.h:25,\n                 from /usr/local/cuda/include/cub/block/specializations/../../block/../block/radix_rank_sort_operations.cuh:36,\n                 from /usr/local/cuda/include/cub/block/specializations/../../block/block_radix_rank.cuh:41,\n                 from /usr/local/cuda/include/cub/block/specializations/../../block/block_radix_sort.cuh:38,\n                 from /usr/local/cuda/include/cub/block/specializations/block_histogram_sort.cuh:36,\n                 from /usr/local/cuda/include/cub/block/block_histogram.cuh:36,\n                 from /usr/local/cuda/include/cub/cub.cuh:43,\n                 from ggml/src/ggml-cuda/sum.cu:8:\n/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \nIn file included from /usr/local/cuda/include/cub/util_arch.cuh:36,\n                 from /usr/local/cuda/include/cub/config.cuh:35,\n                 from /usr/local/cuda/include/cub/cub.cuh:37,\n                 from ggml/src/ggml-cuda/sum.cu:8:\n/usr/local/cuda/include/cub/util_cpp_dialect.cuh:142:13: warning: CUB requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n  142 |      CUB_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                        \nIn file included from /usr/local/cuda/include/thrust/detail/config/config.h:27,\n                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n                 from /usr/local/cuda/include/thrust/type_traits/integer_sequence.h:25,\n                 from /usr/local/cuda/include/cub/block/specializations/../../block/../block/radix_rank_sort_operations.cuh:36,\n                 from /usr/local/cuda/include/cub/block/specializations/../../block/block_radix_rank.cuh:41,\n                 from /usr/local/cuda/include/cub/block/specializations/../../block/block_radix_sort.cuh:38,\n                 from /usr/local/cuda/include/cub/block/specializations/block_histogram_sort.cuh:36,\n                 from /usr/local/cuda/include/cub/block/block_histogram.cuh:36,\n                 from /usr/local/cuda/include/cub/cub.cuh:43,\n                 from ggml/src/ggml-cuda/sum.cu:8:\n/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:131:13: warning: Thrust requires at least C++14. C++11 is deprecated but still supported. C++11 support will be removed in a future release. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n  131 |      THRUST_COMPILER_DEPRECATION_SOFT(C++14, C++11);\n      |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                           \n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install huggingface_hub  # If not already installed\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:58:25.183914Z","iopub.execute_input":"2024-10-10T05:58:25.184735Z","iopub.status.idle":"2024-10-10T05:58:39.036516Z","shell.execute_reply.started":"2024-10-10T05:58:25.184687Z","shell.execute_reply":"2024-10-10T05:58:39.035345Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\n# Download the model from Hugging Face\nmodel_path = snapshot_download(repo_id=\"Jenas-Anton/llama-3-8b-chat-doctor\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:58:42.789164Z","iopub.execute_input":"2024-10-10T05:58:42.789555Z","iopub.status.idle":"2024-10-10T06:15:41.791788Z","shell.execute_reply.started":"2024-10-10T05:58:42.789517Z","shell.execute_reply":"2024-10-10T06:15:41.790836Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aebb7e2ca1054ac8ad08779eff293752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b76c0bca29049289842de8ae8972734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937e6e7ad09248379c2d8de8bade1afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0681a1566d7542758db54064df158667"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9196bf402ee47c2a1c340d2339ba1e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f12d67b357004c368cb4c3f2ed918659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/784 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ecb337ed28448f18856789a26cfa2ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84df844d6eb744b89102211a6843fb8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0364468c1d6d4c3d921a29837fe03fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc761051f2694e6c906c5e55e2d042cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ae7c479aa4423a8c22dbe830a46d98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab52c72b50fe4fb088148bef2c1ea390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6723cb18e14dadb788eeb994b57d1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/419 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d44ff359c874abd8f4991ce29a473ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd4f24382a941229ebda6919d6b4b4b"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py {model_path} \\\n    --outfile /kaggle/working/llama-3-8b-chat-doctor.gguf \\\n    --outtype f16","metadata":{"execution":{"iopub.status.busy":"2024-10-10T06:15:51.663922Z","iopub.execute_input":"2024-10-10T06:15:51.664304Z","iopub.status.idle":"2024-10-10T06:17:07.933792Z","shell.execute_reply.started":"2024-10-10T06:15:51.664263Z","shell.execute_reply":"2024-10-10T06:17:07.932423Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n0it [00:00, ?it/s]\nWriting: 100%|███████████████████████████| 16.1G/16.1G [01:00<00:00, 266Mbyte/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}